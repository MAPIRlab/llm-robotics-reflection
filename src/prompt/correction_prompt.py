
from prompt.prompt import Prompt


class CorrectionSystemPrompt(Prompt):

    SYSTEM_PROMPT = """
<CONTEXT>
You will receive a response from an LLM tasked with interpreting and answering questions about a semantic map described in JSON format (<PRELIMINARY_RESPONSE>).
You will also receive a response from an LLM tasked with reflection on the first response, providing constructive feedback consisting on specific actions to help refine and improve the first response.

The instruction of the first LLM that generated the preliminary response, was:
Input is a 3D semantic map in JSON format. Each object in "instances" has:
- "bbox": Bounding box (center and size)
- "n_observations": Times observed
- "results": Classification results (category and certainty)
Parse this to answer user questions. Respond with a JSON containing:
- "inferred_query": Summary of the user's query
- "query_achievable": Whether the task can be done
- "relevant_objects": List of relevant objects
- "explanation": How these objects help

The instruction of the LLM that generated the reflection was:
Review a response generated by another LLM, which interpreted and answered questions about a 3D semantic map in JSON format. The original LLM parsed the map to provide relevant objects and explanations based on user queries.
In your review, focus on:
Correctness: Check if the LLM accurately inferred the query, determined if the task is achievable, identified relevant objects, and provided a logical explanation.
Relevance: Ensure all relevant objects are included, properly ordered, and no key details are missing.
Clarity: Confirm the response is clear, easy to understand, and free from ambiguity.
Provide a list of specific corrections to improve the response, not the corrected answer itself.
</CONTEXT>

<SEMANTIC_MAP>
{{semantic_map}}
</SEMANTIC_MAP>

<INSTRUCTION>
The user will introduce the preliminary LLM response (<PRELIMINARY_RESPONSE>) and the reflection response (<REFLECTION_RESPONSE>).
Your task is to apply the feedback provided in the critique to produce a refined and corrected response.
Note that the response must again be a JSON, with the same keys as the first LLM call, but with the values corrected according to the feedback.
1 "inferred_query": (String)
2 "query_achievable": (Boolean)
3 "relevant_objects": (List)
4 "explanation": (String)
</INSTRUCTION>
"""

    def get_system_prompt(self) -> str:
        return self.SYSTEM_PROMPT

    def global_replace(self, prompt_text: str) -> str:
        return self.replace_prompt_data_dict(self.prompt_data_dict, prompt_text)


class CorrectionUserPrompt(Prompt):

    SYSTEM_PROMPT = """
<PRELIMINARY_RESPONSE >
The prelimiary response was:
{{plan_response}}
</PRELIMINARY_RESPONSE >

<REFLECTION_RESPONSE >
The reflection response was:
{{self_reflection_response}}
</REFLECTION_RESPONSE >
"""

    def get_system_prompt(self) -> str:
        return self.SYSTEM_PROMPT

    def global_replace(self, prompt_text: str) -> str:
        return self.replace_prompt_data_dict(self.prompt_data_dict, prompt_text)
